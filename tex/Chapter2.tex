% !TEX root = ../thesis.tex
\chapter{基于自适应相似性矩阵的无监督度量学习}
\section{引言}
数据在数据空间中的空间分布学习一直是一个困难的问题。经验上，数据往往分布在高维空间的一个低维流形上，而不是完全随机分布。这导致了不同标签的数据之间可能有较近的欧氏距离。这种分布特性会对基于传统欧氏距离度量的分类或聚类算法的效果产生影响。一个较好的选择是构建一个能够保持数据对之间局部近邻结构的低维映射。

谱聚类方法是一类在基于矩阵特征分解的聚类方法，该类方法在很多具有挑战性的真实世界数据集上表现出了非常优异的性能。在最近几十年间，一系列经典的谱聚类方法被提出，例如：多维标度法（Multidimensional Scaling，MDS）\cite{cox2000multidimensional}，局部线性嵌入（Local Linear Embedding，LLE）\cite{roweis2000nonlinear}，等距特征映射（Isomap）\cite{tenenbaum2000global}，拉普拉斯特征映射（Laplacian Eigenmaps）  \cite{belkin2001laplacian}和变种的谱聚类\cite{ng2002spectral}. 上述提到的谱聚类算法有三点不足之处。第一，这些谱聚类算法只提供了训练数据的嵌入映射，对样本外数据（out-of-sample）的计算比较困难。第二，这些算法的复杂度依赖于数据点数量，相对比较耗时，可扩展性不强。第三，谱聚类算法的稳定性高度依赖于相似性图（affinity graph）的鲁棒性。

为缓解上述谱聚类中存在的问题，大量重要的研究进展被提出\cite{bengio2004out,niyogi2004locality,fowlkes2004spectral,yan2009fast,chen2011large,pavan2007dominant,premachandran2013consensus,zhu2014constructing,nie2014clustering}。局部保持投影（Locality Preserving Projections， LPP）\cite{niyogi2004locality}引入了一种由拉普拉斯特征映射得到的线性投影方法。他们的工作提供了一种嵌入映射的线性近似，该线性近似可以减少计算复杂度并可以简单的实现样本外数据的扩展。线性嵌入提供了一种度量学习角度下的谱聚类方法。文献\parencite{nie2014clustering}提出了自适应近邻投影聚类（Projected Clustering with Adaptive Neighbors，PCAN）算法，该算法将点对之间的相似性作为一个额外的待求解变量并且通过对图拉普拉斯（graph Laplacian）矩阵的秩设置惩罚项以限制相似性矩阵的连通区数量。基于这个框架，PCAN算法交替地更新相似性矩阵和投影。经验上，流形嵌入方法的效果依赖于相似性矩阵的鲁棒性。图\ref{fig2:affMat}给出了MNIST数据集\cite{lecun1998gradient}的子集在理想情况下及不同近邻数量下的热力核相似性矩阵。从图中可看出广泛使用的$k$近邻（$k$ nearest neighborhood，$k$-NN）热力核中存在大量的噪声。虽然一些相似性学习的方法已经被提出，选择合适的相似性矩阵的问题依然有待解决。

\begin{figure}[!htbp]
	\centering
	\bisubcaptionbox{理想的相似性矩阵}%
					{Ideal affinity matrix}
					[0.3\textwidth]{\includegraphics[width=0.3\textwidth]{chap2/k1.jpg}}
	\hspace{1em}
	\bisubcaptionbox{$k=100$下的热力核相似性矩阵}%
					{Affinity matrix of heat kernel with $k=100$}
					[0.3\textwidth]{\includegraphics[width=0.3\textwidth]{chap2/k100.jpg}}
	\hspace{1em}
	\bisubcaptionbox{$k=1000$下的热力核相似性矩阵}%
					{Affinity matrix of heat kernel with $k=1000$}%
					[0.3\textwidth]{\includegraphics[width=0.3\textwidth]{chap2/k1000.jpg}}
	\bicaption{MNIST子集在理想情况及不同近邻数量$k$下的相似性矩阵。}
			  {Ideal affinity matrix and affinity matrices with different neighborhood size $k$ on a subset of MNIST}
  	\label{fig2:affMat}
\end{figure}

本章所提出的方法的目标在于，针对谱聚类的线性近似，在最小的时间消耗下，提取更具有自适应性的相似性信息。这类信息将会更多的考虑局部性保持（locality preserving）这个优化目标，而不仅仅是图像间的距离。受到可扩展谱聚类和数据相似性学习方向上的研究成果\cite{chen2011large,nie2014clustering}的启发，本章提出了一个新颖的自适应相似性矩阵（Adaptive Affinity Matrix，AdaAM）方法。该方法的相似性矩阵相对稠密并可以同时捕捉到全局及局部信息。特别地，AdaAM将相似性矩阵分解成两个相同的低秩矩阵的乘积。作为文献\parencite{ng2002spectral}中描述的理想情况，如果我们假设同一个类内的点对相似性极为相似，则相似性矩阵可能会形成一个低秩矩阵。本算法采用与谱聚类方法相似的优化方法对分解后的矩阵进行优化求解。优化得到的相似性图作为最终结果前的中间态相似性矩阵。通过合并基于热力核获得的$k$-NN相似性图和中间态相似性矩阵，AdaAM采用朴素的谱聚类求解方法计算出一个最终的自适应相似性矩阵。我们将LPP方法作用于此特定的相似性矩阵上进行数据投影，以获得一个用于聚类的距离度量。

在本章中，我们将通过在图像数据集上的聚类实验展示所提出方法的有效性和高效性。第 \ref{sec2:Exp}节通过对本章提出的算法和基于$k$-NN的拉普拉斯特征映\cite{belkin2001laplacian}以及其他最先进方法进行对比展现了AdaAM方法在具有挑战性的数据集上的聚类效果具有明显优势。


\section{自适应相似性矩阵}
在本节中，我们首先对本节采用的数学符号进行说明，其次介绍中间态相似性矩阵的计算方法，然后描述最终自适应相似性矩阵的获取方式，最后对算法流程中的稀疏化策略进行介绍。
\subsection{符号说明}
在本章中，我们以大写英文或希腊字母代表矩阵，小写字母代表向量。所有元素全部为$1$的向量用$\bf{1}$表示。$H$表示中心化矩阵（centering matrix），其定义为：$H = I-\frac{1}{n}\bf{1}\bf{1}^T$。原始数据矩阵以$X \in \mathbb{R}^{n\times d}$表示，其中$n$为数据点的数量，$d$为数据维数。本章中将假设数据矩阵$X$为零均值归一化矩阵，即$X = HX$。符号$x_i$表示数据矩阵$X$中的第$i$个数据点向量。我们用$A$表示线性投影，相应的距离度量矩阵表示为$M = A^TA$。因此，在该度量矩阵下的距离定义为$dis_m(x_i, x_j) = (x_i - x_j)^TM(x_i - x_j)$。我们将$k$-NN热力核矩阵表示为$W \in \mathbb{R}^{n \times n}$：
\begin{equation}
	w_{ij} = \begin{cases} \mathrm{exp}(-\frac{\|x_i-x_j\|_{2}}{t}), \; &x_i\in\mathcal{N}_k(x_j)\;\mathrm{or}\; x_j\in\mathcal{N}_k(x_i);\\
		0, &\mathrm{otherwise.}\end{cases}          
\end{equation}
此处，$\mathcal{N}_k(x)$表示数据点$x$的 $k$ 个最近邻的集合。相应的拉普拉斯矩阵表示为$L=D-W$，其中$D$为对角权重矩阵：$d_{ii} = \sum_j w_{ij}$。我们对中间态增量矩阵及最终自适应相似性矩阵都采用$\Delta$表示，相应的对角权重矩阵和拉普拉斯矩阵分别表示为$D_\Delta$和$L_\Delta = D_\Delta-\Delta$。

\subsection{中间态相似性矩阵}
我们将提出的AdaAM算法分为两个部分，分别是中间态相似性矩阵和最终自适应相似性矩阵。本节将对其中的中间态相似性矩阵部分的生成进行介绍。对于第$i$个数据点$x_i$，我们使用相似度$\delta_{ij}$对任意的数据点$x_i$和数据点$x_j$进行连接。由于希望任意两个数据点之间较近的欧氏距离能导致较大的数据点相似度，我们的目标为通过选择最优的相似度$\delta_{ij}$在特定合适的约束下最小化下述的目标函数，
\begin{equation}
	\mathop{\mathrm{min}} \sum_{i,j}^{n} \|x_i-x_j\|_2^2\;\delta_{ij},
\end{equation}
在这里，$\delta_{ij}$表示中间态相似性矩阵$\Delta$中第$ij$个元素。

不同于PCAN算法\cite{nie2014clustering}，我们基于图拉普拉斯对该目标函数公式在其约束条件下进行变形，
\begin{equation}
	\mathop{\mathrm{min}}\; tr(X^TL_\Delta X).
	\label{eq2:Delta}
\end{equation}

由于在通常情况下图拉普拉斯为半正定矩阵，因此一个直接又看似可行的思路是将拉普拉斯矩阵分解为两个相同矩阵的乘积。在下文中我们将论述该思路不适用于我们的计算框架。

我们采用$U\in \mathbb{R}^{n\times s}$ 表示一个列正交矩阵，以满足$U^TU = I$，则我们可以假设
\begin{equation}
	L_\Delta = UU^T.
\end{equation}
在对原优化问题进行约束松弛的情况下，我们最终需要求解问题为：
\begin{equation}
	% \begin{split}
		U = \mathop{\mathrm{arg\;min}}_{U^TU=I}\; tr(X^TUU^TX) 
		\Rightarrow U = \mathop{\mathrm{arg\;min}}_{U^TU=I} \;tr(U^TXX^TU)
	% \end{split}
	\label{eq2:simpEigmap}
\end{equation}

如果我们假设矩阵$X$的乘积为矩阵$K$，即$K = XX^T$，则公式（\ref{eq2:simpEigmap}）所得为一个简单形式的拉普拉斯特征映射\cite{belkin2001laplacian}。
由此，该优化问题可以通过选择矩阵$K$的多个最小特征值所对应的特征向量形成矩阵进行求解。但是，由于$K$为一个低秩矩阵，一般情况下满足条件$d\ll n$，$K$可以最小化公式（\ref{eq2:simpEigmap}）中目标函数的特征向量都存在于数据矩阵$X$的零空间中。
因此，上述问题的最优解不唯一。受LSC（Landmark-based Spectral Clustering）算法\cite{chen2011large}启发，我们将相似性矩阵假设为一个半正定矩阵。不同于对拉普拉斯矩阵进行分解，我们将半正定的相似性矩阵分解为列正交矩阵 $P\in \mathbb{R}^{n\times t}$及其转置矩阵$P^T$的乘积，其中 $t$ 为预期下的矩阵$\Delta$的秩。

由此，我们可以将公式（\ref{eq2:Delta}）改写为
\begin{equation}
	% 	\begin{split}
	% 		&\mathop{\mathrm{min}}_{P^TP=I} tr(X^T(D_\Delta-PP^T)X) \\
	% 		&\Rightarrow 
	\mathop{\mathrm{min}}_{P^TP=I} tr(X^TD_\Delta X)+tr(X^T(-PP^T)X),
	% 	\end{split}
	\label{eq2:XLX}
\end{equation}
此处，我们舍弃了连接权重为非负相似度以及图拉普拉斯为半正定矩阵这两个特性。相似性矩阵$\Delta$中的负值连接权重可以用于度量数据点间的不相似度。在下文中，我们将展示此优化问题的最优解将使$D_\Delta$ 等于 $\bf0$。

对于公式（\ref{eq2:XLX}）中的第一部分，我们可以将其写为
\begin{equation}
	\begin{split}
		\mathop{\mathrm{min}}\;&\sum_{i=1}^{n} \|x_i\|_2^2\;d_{\Delta ii},\\
		s.t. \;\; &P^TP=I;\\
		&d_{\Delta ii}=(PP^T\textbf{1})_i.
	\end{split}
	\label{eq2:XD}
\end{equation}

令$z=(\|x_1\|_2^2, \|x_2\|_2^2, ... , \|x_n\|_2^2)^T$。此处，取拉格朗日乘子为$\lambda$，公式（\ref{eq2:XD}）在一维情况下的优化问题可以表示为
\begin{equation}
	\mathop{\mathrm{min}}\; z^Tpp^T\textbf{1}-\lambda (p^Tp-1)
\end{equation}

最终，最小化优化问题（\ref{eq2:XD}）可以被化简为求解问题
\begin{equation}
	\textbf{1}z^Tp=\lambda p
\end{equation}
中最小特征值对应地特征向量。因为矩阵$\textbf{1}z^T$为秩$1$矩阵，所以只存在一个非零特征值$\sum_{i=1}^{n}\|x_i\|_2^2$，同时这也意味着 $\lambda = 0$。由此可知，对于具有任意的小于 $n$的列数且满足优化问题（\ref{eq2:XD}）的列正交矩阵$P$,我们可以得出$z^TPP^T\textbf{1}=0$。这个结论等价于
\begin{equation}
	\sum_{i=1}^{n} \|x_i\|_2^2\;d_{\Delta ii} = 0.
\end{equation}

一般性地，对于真实世界数据集中数据点$x_i$，$\|x_i\|_2^2\neq0$恒成立，故能够最小化目标函数（\ref{eq2:XLX}）第一部分的最优解$P$ 具有性质$D_\Delta = \textbf{0}$。同时可知，具有性质$D_\Delta = \textbf{0}$的所有$P$的集合构成优化问题（\ref{eq2:XD}）的解集。

最小化优化目标函数（\ref{eq2:XLX}）的第二部分的矩阵$P$，可以通过求解下述特征问题中最大特征值获得：
\begin{equation}
	\begin{split}
		(XX^T)p = \lambda p
		\Rightarrow {\bf1}XX^Tp = \lambda {\bf1}p
	\end{split}
	\label{eq2:Solu}
\end{equation}

由数据矩阵$X$为零均值归一化矩阵，可以得知$\lambda {\bf1}p = {\bf1}XX^Tp = 0$。因而，对于大于$0$的最大特征值，其对应地特征向量$p$总满足${\bf1}p = 0$。将令公式（\ref{eq2:XLX}）中第二部分取得最小值的解$P$写为 $P=(p_1,p_2,...,p_t)$，可以得到
\begin{equation}
	%\begin{split}
	{\bf1}^T P = {\bf0} \Rightarrow {\bf1}^T PP^T = {\bf0} \Rightarrow d_{\Delta ii} = 0. %\quad(i=1,...,n)
	%\end{split}
\end{equation}
由此可知，特性$D_\Delta = \textbf{0}$对于公式（\ref{eq2:XLX}）第二部分的最优解恒成立，并且该最优解属于公式（\ref{eq2:XD}）的解集。所以，公式（\ref{eq2:XLX}）第二部分的最优解同时可以使目标函数（\ref{eq2:XD}）达到最优。完整的优化问题（\ref{eq2:XLX}）的最优解使得$D_\Delta$ 等于$\bf0$。因此，优化目标函数（\ref{eq2:XLX}）可以进一步化简为
\begin{equation}
	P=\mathop{\mathrm{arg\;max}}_{P^TP=I} \;tr(P^TXX^TP).
	\label{eq2:PXXP}
\end{equation}
公式（\ref{eq2:PXXP}）的最优解可以通过对数据矩阵 $X$ 的奇异值分解求得，其运算复杂度依赖于数据维度$d$而不是数据量$n$。至此，我们从同时具有相似度和不相似度信息的原始数据分布中获得了中间态相似性矩阵 $\Delta = PP^T$。矩阵$\Delta$所对应的图拉普拉斯矩阵为$L_\Delta = D_\Delta-\Delta =-\Delta $。

为了能够缓解计算中的噪声影响以及矩阵秩减少的问题，我对矩阵$\Delta$进行了稀疏化处理。我们将在第\ref{sec2:sparse}节对稀疏化处理的细节进行进一步讨论。

\subsection{最终自适应相似性矩阵}
在本节中，我们对朴素的线性谱聚类进行公式化，并且给出最终自适应相似性矩阵的求解方法。

在已经求得中间态相似性矩阵$\Delta$的情况下，我们可以通过求解下述问题获得一个线性投影矩阵$A$：
\begin{equation}
	a = \mathop{\mathrm{arg\;min}}_{a^Ta=1}\; tr(a^TX^T(L+L_\Delta)Xa),
	\label{eq2:AXLXA}
\end{equation}
此处，$a$ 为矩阵$A$的一维情况，且$L+L_\Delta$是$k$-NN热力核与中间态相似性矩阵的拉普拉斯矩阵的组合。投影向量$a$可通过求解特征分解问题的最小特征值得到：
\begin{equation}
	X^T(L-\Delta)Xa = \lambda a
\end{equation}

然后，为在给定$A$的情况下求解公式（\ref{eq2:AXLXA}）中的$L_\Delta$，我们依照对公式（\ref{eq2:XLX}）的变形，将相似性优化问题进行重写并引入线性投影矩阵$A$：
\begin{equation}
	% \begin{split}		
		P = \mathop{\mathrm{arg\;min}}_{P^TP=I}\; \Big( c+tr(A^TX^TD_\Delta XA) 
		+tr(A^TX^T(-PP^T)XA)\Big),
	% \end{split}
	\label{eq2:AXDXA-PXAAXP}
\end{equation}
这里$c=tr(A^TX^TLXA)$，并且我们假设最终自适应相似性矩阵为$\Delta = PP^T$。由于$X$为列零均值归一化矩阵，则$XA$同样为列零均值矩阵，特性$D_\Delta = \textbf{0}$依然成立。由此可知，公式（\ref{eq2:AXDXA-PXAAXP}）可化简为
\begin{equation}
	P = \mathop{\mathrm{arg\;max}}_{P^TP=I}\; tr(P^TXAA^TX^TP)
	\label{eq2:PXAAXP}
\end{equation}

公式（\ref{eq2:PXAAXP}）可以通过对矩阵$A$进行奇异值分解并提取前$t$个最大奇异值对应的左奇异向量（left-singular vectors）进行求解。我们对从公式（\ref{eq2:PXAAXP}）求解得到的自适应相似性矩阵$\Delta=PP^T$进行稀疏化处理，由此可以得到稀疏相似性矩阵。

直观来说，我们可以通过对公式（\ref{eq2:AXLXA}）和公式（\ref{eq2:PXAAXP}）进行交替迭代求解以使目标函数最小化。然而，如图\ref{fig2:itera}所示，在实际计算中，仅在执行一次迭代运算后所得到的自适应相似性矩阵即可取得很好的聚类效果。持续的迭代求解并没有表现出显著的聚类准确率提升。

在一些算法中图节点的权重具有重要作用，同时例如LPP的一类基于归一化割（Normalized Cuts，NCuts）的方法存在依赖于矩阵$D_\Delta$的优化约束条件。然而在我们的算法中有$D_\Delta = \textbf{0}$，因此，我们将由原始$k$-NN热力核计算得到的权重矩阵$D$叠加到求得的自适应相似性矩阵上，以还原损失的权重信息。最后，我们将原LPP算法中的相似性矩阵替换为由AdaAM方法求得的矩阵$\Delta+D$即可求解出线性投影矩阵$A$ 以及距离度量矩阵$M = A^TA$。

\begin{algorithm}[!htbp]
	\SetKwInput{KwData}{{输入:}
	\SetKwInput{KwResult}{输出:}
	\caption{自适应相似性矩阵}
	\label{alg1}
	\KwData{数据点 $X \in \mathbb{R}^{n \times d}$；聚类数量 $c$；近邻数量 $k$；降维后维度 $m$。}
	\KwResult{距离度量矩阵 $M$ 和线性投影矩阵$A$。}
	构建$k$-NN热力核矩阵$W$，相应的对角化权重矩阵$D$ 以及拉普拉斯矩阵$L$；\\
	根据公式（\ref{eq2:PXXP}），计算获得列正交矩阵$P$，并求得相应的中间态相似性矩阵$\Delta = PP^T$；\\
	根据公式（\ref{eq2:AXLXA}），求解得到线性投影矩阵$A$；\\
	求解公式（\ref{eq2:PXAAXP}），构造新的$P$矩阵，生成相应的最终自适应相似性矩阵$\Delta = PP^T$;\\
	通过将LPP算法作用于相似性矩阵$\Delta + D$，计算出线性投影矩阵 $A\in\mathbb{R}^{m\times d}$ 及距离度量矩阵 $M=A^TA$。
\end{algorithm}

\begin{figure}[!htbp]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chap2/itera_data.pdf}
		\caption{}
		\label{fig2:itera}
	\end{subfigure}
	% \hspace{1em}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{chap2/delta_hist.pdf}
		\caption{}
		\label{fig2:Hist}
	\end{subfigure}
	\bicaption{（a）在数据集USPS上不同的迭代计算次数的聚类效果评价结果。迭代计算所贡献的精度提升小于0.5\%。（b）数据集USPS的最终自适应相似性矩阵的元素强度直方图。}
			  {(a)The evaluation of the clustering performance with different times iterative computation on the data set USPS. The contribution to accuracy made by iteration is less than 0.5\%. (b)The histogram of the element magnitude of the final adaptive affinity matrix obtained from data set USPS. }
\end{figure}

\subsection{稀疏化策略}
\label{sec2:sparse}
\section{实验结果}
\label{sec2:Exp}



\begin{table}[!htbp]
	\bicaption{不同图像数据集上的聚类准确率（\%）}
	{Clustering accuracy on different image datasets(\%)}
	\label{tab2:Acc}
	\centering
	% \begin{small}
		%\begin{sc}
		\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multicolumn{1}{|l||}{} &\multicolumn{2}{|c|}{\bf AdaAM} &\multicolumn{2}{|c|}{\bf $k$-NN} &\multicolumn{2}{|c|}{\bf Cons-$k$NN} &\multicolumn{2}{|c|}{\bf DN} &\multicolumn{2}{|c|}{\bf ClustRF-Bi} & \multicolumn{2}{|c|}{\bf PCAN-$k$Means} & \multicolumn{1}{|c|}{\bf PCAN}
			\\ \hline
			& Avg & Max & Avg & Max & Avg & Max & Avg & Max & Avg & Max & Avg & Max & \\
			\hline
			UMIST    & \textbf{66.06}& \textbf{75.65}& 58.16& 65.39& 60.27& 69.22& 59.15& 66.96& 64.63& 74.44& 53.79& 56.52& 55.30\\
			COIL20   & 74.72& \textbf{87.29}& 71.89& 81.18& 75.53& 84.31& 71.95& 82.01& \textbf{76.50}& 85.07& 72.28& 83.75& 81.74\\
			USPS     & \textbf{69.36}& \textbf{69.61}& 68.25& 68.35& 68.21& 68.34& 68.08& 68.31& 58.74& 65.90& 64.04& 67.95& 64.20\\
			MNIST    & \textbf{60.84}& \textbf{61.34}& 48.13& 48.27& 47.88& 48.00& 49.72& 49.76& 51.93& 52.03& 58.93& 58.98& 59.83\\
			ExYaleB  & \textbf{54.36}& \textbf{57.87}& 24.17& 26.76& 25.63& 28.75& 24.21& 27.42& 23.10& 26.43& 25.74& 27.63& 25.89\\
			\hline
		\end{tabular}
		%\end{sc}
	% \end{small}
\end{table}